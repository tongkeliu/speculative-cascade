import argparse
import torch
import time
import os
import logging
import contexttimer


def args_parser():
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--gamma", type=int, default=20, help="tokens generated by SLM",
    )
    parser.add_argument(
        "--alpha", type=float, default=0.5, help="cost of switching to target model"
    )
    parser.add_argument(
        "--approx_model",type=str,default="qwen3-0.6b",help="a small model to approximate large one",
    )
    parser.add_argument(
        "--target_model", type=str, default="qwen3-8b", help="the target model"
    )
    parser.add_argument(
        "--deferral_rule", type=str, default='chow', help="select the deferral rule"
    )

    parser.add_argument(
        "--use_kvcache", type=bool, default=True, help="if use kvcache or not"
    )
    parser.add_argument(
        "--top_k", type=int, default=50, help="top_k value"
    )
    parser.add_argument(
        "--top_p", type=float, default=0.9, help="top_p value"
    )
    parser.add_argument(
        "--temperature", type=float, default=0.7, help="temperature"
    )
    
    parser.add_argument(
        "--max_length", type=int, default=512, help="max length generated by model"
    )
    parser.add_argument(
        "--prompt", type=str, default="Once upon a time, there"
    )
    parser.add_argument(
        "--seed", type=int, default=222, help="set seed to ensure reproduction"
    )
    parser.add_argument(
        "--device", type=str, default="cuda:3", help="GPU index"
    )
    args = parser.parse_args()
    return args


def get_logger(args):
    base_dir = 'save/logs'
    if not os.path.exists(base_dir):
        os.makedirs(base_dir)
    current_time = time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time()))
    details = f"{args.approx_model}-{args.target_model}-{current_time}.log"
    log_dir = os.path.join(base_dir, details)
    logger = logging.getLogger('logger')
    logger.setLevel(logging.DEBUG)

    file_handler = logging.FileHandler(log_dir)
    file_handler.setLevel(logging.INFO)
    file_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
    file_handler.setFormatter(file_formatter)

    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)
    console_formatter = logging.Formatter('%(asctime)s %(levelname)s: %(message)s')
    console_handler.setFormatter(console_formatter)

    logger.addHandler(file_handler)
    logger.addHandler(console_handler)

    return logger


def inference_speed(fn, sample_num, logger, method):
    token_sum = 0
    with contexttimer.Timer() as t:
        for i in range(sample_num):
            res = fn()
            token_sum += res.shape[1]
    logger.info(f"[{method}] {token_sum / (sample_num * t.elapsed)} tokens/sec")


def norm_top_k_top_p_filter(temperature, logits, top_k, top_p): # copyed
    logits = logits / temperature
    if top_k > 0:
        filter = torch.topk(logits, min(top_k, logits.size(-1)))[0]
        logits[logits < filter[:, [-1]]] = float('-inf')
    if top_p > 0.0:
        sorted_logits, sorted_indices = torch.sort(logits, descending=True)
        cumulative_probs = torch.cumsum(
            torch.softmax(sorted_logits, dim=-1), dim=-1)
        filter = cumulative_probs > top_p
        filter[..., 1:] = filter[..., :-1].clone()
        filter[..., 0] = 0
        indices_to_remove = filter.scatter(1, sorted_indices, filter)
        logits[indices_to_remove] = float('-inf')
    logits = torch.softmax(logits, dim=1)
    return logits


def max_fn(probs):
    probs_max = torch.where(probs > 0, probs, torch.zeros_like(probs))
    # probs_sum = torch.sum(probs_max, dim=1, keepdim=True)
    # return probs / (probs_sum + 1e-8)
    return probs_max


def deferral_rule(rule, q, p, alpha):
    if rule == 'chow':
        q_max = torch.max(torch.softmax(q, dim=1))
        return q_max < 1 - alpha
    elif rule == 'diff':
        q_max, p_max = torch.max(q), torch.max(p)
        return q_max < p_max - alpha