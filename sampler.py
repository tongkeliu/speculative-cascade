import torch
from modelscope import AutoTokenizer, AutoModelForCausalLM
from transformers import DynamicCache
from utils import *

model_collection = {
    "qwen3-0.6b":"Qwen/Qwen3-0.6B",
    "qwen3-4b":"Qwen/Qwen3-4B",
    "qwen3-8b":"Qwen/Qwen3-8B"
}

class Sampler():
    def __init__(self, args):
        self.args = args
        self.tokenizer = AutoTokenizer.from_pretrained(
            model_collection[args.approx_model],
            dtype="auto",
            device_map=args.device,
            trust_remote_code=True  
        )
        self.approx_model = AutoModelForCausalLM.from_pretrained(
            model_collection[args.approx_model],
            dtype="auto",
            device_map=args.device,
            trust_remote_code=True  
        )
        self.target_model = AutoModelForCausalLM.from_pretrained(
            model_collection[args.target_model],
            dtype="auto",
            device_map=args.device,
            trust_remote_code=True          
        )
    
    @torch.no_grad()
    def autoregressive(self):
        model = self.target_model
        past_key_values = None

        for i in range(self.args.max_length):
            if past_key_values == None:
                input = self.tokenizer(self.args.prompt, return_tensors="pt").to(self.args.device)
                result_ids = input["input_ids"]
                output = model(**input, use_cache=True)
            else:
                output = model(input_ids=sampled_token, use_cache=True, past_key_values=past_key_values)

            past_key_values = output.past_key_values
            logits = norm_top_k_top_p_filter(self.args.temperature, output.logits[:,-1,:],\
                                             self.args.top_k, self.args.top_p)
            sampled_token = torch.multinomial(logits, num_samples=1)
            result_ids = torch.cat((result_ids, sampled_token), dim=1)

        past_key_values = None # reset kvcache
        return result_ids
    
    @torch.no_grad()
    def speculative(self):
        ap_past_key_values = DynamicCache()
        tg_past_key_values = DynamicCache()
        ap_prob_history = None
        tg_prob_history = None
        ap_input_ids = self.tokenizer(self.args.prompt, return_tensors="pt")["input_ids"].to(self.args.device)
        n = ap_input_ids.shape[1] - 1
        result_ids = ap_input_ids
        reject_cnt = 0

        while n < self.args.max_length:
            # approx_model generate gamma tokens autoregressively
            for i in range(self.args.gamma):
                if ap_past_key_values.get_seq_length() == 0:
                    ap_output = self.approx_model(input_ids=ap_input_ids, use_cache=True, 
                                                  past_key_values=ap_past_key_values)
                    ap_prob_history = ap_output.logits
                else:
                    ap_output = self.approx_model(input_ids=sampled_token, use_cache=True, 
                                                  past_key_values=ap_past_key_values)
                    ap_prob_history = torch.cat((ap_prob_history, ap_output.logits), dim=1)

                logits = norm_top_k_top_p_filter(self.args.temperature, ap_output.logits[:,-1,:],\
                                                 self.args.top_k, self.args.top_p)
                sampled_token = torch.multinomial(logits, num_samples=1)
                result_ids = torch.cat((result_ids, sampled_token), dim=1)
            
            # target_model(verifier) check stage
            if tg_past_key_values.get_seq_length() == 0:
                tg_output = self.target_model(input_ids=result_ids[:,:-1], use_cache=True, 
                                            past_key_values=tg_past_key_values)
                tg_prob_history = tg_output.logits
            else:                
                tg_output = self.target_model(input_ids=result_ids[:,-(self.args.gamma+1):-1], 
                                              use_cache=True, past_key_values=tg_past_key_values)
                tg_prob_history = torch.cat((tg_prob_history, tg_output.logits), dim=1)

            for i in range(self.args.gamma):
                token = result_ids[0][n+i+1]
                # reject condition
                randnum = torch.rand((1, 1), device=self.args.device)
                if randnum < 1 - tg_prob_history[:,n+i,token] / ap_prob_history[:,n+i,token]:
                    probs = max_fn(tg_prob_history[:,n+i,:] - ap_prob_history[:,n+i,:])
                    sampled_token = torch.multinomial(probs, num_samples=1)

                    n = n + i
                    reject_cnt += 1
                    ap_past_key_values.crop(n)
                    tg_past_key_values.crop(n)
                    result_ids = result_ids[:, :n + 1]
                    ap_prob_history = ap_prob_history[:, :n]
                    tg_prob_history = tg_prob_history[:, :n]
                    break
            else:
                # accept all token generated by approx model
                n = n + self.args.gamma
                continue
        
        ap_past_key_values.reset()
        tg_past_key_values.reset()
        print("reject_cnt", reject_cnt)
        return result_ids

    @torch.no_grad()
    def cascade(self):
        ap_past_key_values = DynamicCache()
        tg_past_key_values = DynamicCache()
        ap_input_ids = self.tokenizer(self.args.prompt, return_tensors="pt")["input_ids"].to(self.args.device)
        result_ids = ap_input_ids
        switch_cnt = 0
        for i in range(self.args.max_length):
            if ap_past_key_values.get_seq_length() == 0:
                ap_output = self.approx_model(input_ids=ap_input_ids, use_cache=True, 
                                                past_key_values=ap_past_key_values)
            else:
                ap_output = self.approx_model(input_ids=sampled_token, use_cache=True, 
                                                past_key_values=ap_past_key_values)
            output = ap_output
            # we don't implement oracle rule here
            r = deferral_rule(self.args.deferral_rule, ap_output.logits[:, -1, :],
                             None, self.args.alpha)
            if r == 1:
                # switch to target model
                if tg_past_key_values.get_seq_length() == 0:
                    tg_output = self.target_model(input_ids=result_ids, use_cache=True, 
                                                past_key_values=tg_past_key_values)
                else:                
                    tg_output = self.target_model(
                        input_ids=result_ids[:,tg_past_key_values.get_seq_length():], 
                        use_cache=True, 
                        past_key_values=tg_past_key_values
                    )
                output = tg_output
                switch_cnt += 1
            logits = norm_top_k_top_p_filter(self.args.temperature, output.logits[:,-1,:],\
                                                self.args.top_k, self.args.top_p)
            sampled_token = torch.multinomial(logits, num_samples=1)
            result_ids = torch.cat((result_ids, sampled_token), dim=1)

        ap_past_key_values.reset()
        tg_past_key_values.reset()
        print("switch_cnt", switch_cnt)
        return result_ids

    def spec_cascade(self):
        pass
